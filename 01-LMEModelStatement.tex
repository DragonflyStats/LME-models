\section{Statement of the LME model}

Further to a paper published by Laird and Ware in $1982$, a linear mixed effects model is a linear mdoel that combined fixed and random effect terms formulated as follows;

  \begin{displaymath}
      Y_{i} =X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
  \end{displaymath}
\begin{itemize}

\item $Y_{i}$ is the $n \times 1$ response vector \item $X_{i}$ is
the $n \times p$ Model matrix for fixed effects \item $\beta$ is
the $p \times 1$ vector of fixed effects coefficients \item
$Z_{i}$ is the $n \times q$ Model matrix for random effects \item
$b_{i}$ is the $q \times 1$ vector of random effects coefficients,
sometimes denoted as $u_{i}$ \item $\epsilon$ is the $n \times 1$
vector of observation errors
\end{itemize}

%===========================================================================%
It is important to note the following characteristics of this model.
\begin{itemize}
\item Let the number of replicate measurements on each item $i$ for both methods be $n_i$, hence $2 \times n_i$ responses. However, it is assumed that there may be a different number of replicates made for different items. Let the maximum number of replicates be $p$. An item will have up to $2p$ measurements, i.e. $\max(n_{i}) = 2p$.

% \item $\boldsymbol{y}_i$ is the $2n_i \times 1$ response vector for measurements on the $i-$th item.
% \item $\boldsymbol{X}_i$ is the $2n_i \times  3$ model matrix for the fixed effects for observations on item $i$.
% \item $\boldsymbol{\beta}$ is the $3 \times  1$ vector of fixed-effect coefficients, one for the true value for item $i$, and one effect each for both methods.

\item Later on $\boldsymbol{X}_i$ will be reduced to a $2 \times 1$ matrix, to allow estimation of terms. This is due to a shortage of rank. The fixed effects vector can be modified accordingly.
\item $\boldsymbol{Z}_i$ is the $2n_i \times  2$ model matrix for the random effects for measurement methods on item $i$.
\item $\boldsymbol{b}_i$ is the $2 \times  1$ vector of random-effect coefficients on item $i$, one for each method.
\item $\boldsymbol{\epsilon}$  is the $2n_i \times  1$ vector of residuals for measurements on item $i$.
\item $\boldsymbol{G}$ is the $2 \times  2$ covariance matrix for the random effects.
\item $\boldsymbol{R}_i$ is the $2n_i \times  2n_i$ covariance matrix for the residuals on item $i$.
\item The expected value is given as $\mbox{E}(\boldsymbol{y}_i) = \boldsymbol{X}_i\boldsymbol{\beta}.$ \citep{hamlett}
\item The variance of the response vector is given by $\mbox{Var}(\boldsymbol{y}_i)  = \boldsymbol{Z}_i \boldsymbol{G} \boldsymbol{Z}_i^{\prime} + \boldsymbol{R}_i$ \citep{hamlett}.
\end{document} 


\section{Normal linear mixed models}

The standard linear mixed effects model specifies
\begin{equation}
y = X \beta + Zb + \epsilon , 
\label{lme:Model}
\end{equation}
where $y$ is a vector of $N$ observable random variables, $\beta$ is a vector of $p$ unknown parameters having fixed values (fixed effects), $X$ and $Z$ are $N \times p$ and $N \times q$ known matrices, and $b$ and $\epsilon$  are vectors of $q$ and $N,$ respectively, unobservable random variables (random effects) such that $\mathrm{E}(b)=0, \ \mathrm{E}(\epsilon)=0$
and
\[
\mathrm{var}
\pmatrix{
  b \cr
  \epsilon }  =
\pmatrix{
  D & 0 \cr
  0 & \Sigma }
\]
where $D$ and $\Sigma$ are positive definite matrices parameterized by an unknown variance component parameter vector $ \theta.$
%----------------------------------------------------------------------------------------%
%----------------------------------------------------------------------------------------%
\newpage
\section{Statement of the LME model}

Further to a paper published by Laird and Ware in $1982$, a linear mixed effects model is a linear model that combined fixed and random effect terms formulated as follows;

  \begin{displaymath}
      Y_{i} =X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
  \end{displaymath}
\begin{itemize}

\item $Y_{i}$ is the $n \times 1$ response vector \item $X_{i}$ is the $n \times p$ Model matrix for fixed effects \item $\beta$ is the $p \times 1$ vector of fixed effects coefficients \item $Z_{i}$ is the $n \times q$ Model matrix for random effects \item $b_{i}$ is the $q \times 1$ vector of random effects coefficients,
sometimes denoted as $u_{i}$ \item $\epsilon$ is the $n \times 1$ vector of observation errors
\end{itemize}


\section{The Linear Mixed Effects Model}
The linear mixed effects model is given by
\begin{equation}
Y = X\beta + Zu + \epsilon
\end{equation}


\textbf{Y} is the vector of $n$ observations, with dimension $n
\times 1$. \textbf{b} is a vector of fixed $p$ effects, and has
dimension $p \times 1$. It is composed of coefficients, with the
first element being the population mean.  \textbf{X} is known as
the design `matrix', model matrix for fixed effects, and comprises
$0$s or $1$s, depending on whether the relevant fixed effects have
any effect on the observation is question. \textbf{X} has
dimension $n \times p$. \textbf{e} is the vector of residuals with
dimension $n \times 1$.

The random effects models can be specified similarly. \textbf{Z}
is known as the `model matrix for random effects', and also
comprises $0$s or $1$s. It has dimension $n \times q$. \textbf{u
}is a vector of random $q$ effects, and has dimension $q \times
1$.


% \subsection{Formulation of the Variance Matrix V}
\textbf{V} , the variance matrix of \textbf{Y}, can be expressed
as follows;
\begin{eqnarray}
\textbf{V}= var ( \textbf{Xb} + \textbf{Zu} + \textbf{e})\\
\textbf{V}= var ( \textbf{Xb} ) + var (\textbf{Zu}) +
var(\textbf{e}))
\end{eqnarray}

$\mbox{var}(\textbf{Xb})$ is known to be zero. The variance of the
random effects $\mbox{var}(\textbf{Zu})$ can be written as
$Z\mbox{var}(\textbf{u})Z^{T}$.

By letting var$(u) = G$ (i.e $\textbf{u} ~ N(0,\textbf{G})$), this
becomes $ZGZ^{T}$. This specifies the covariance due to random
effects. The residual covariance matrix $var(e)$ is denoted as
$R$, ($\textbf{e} ~ N(0,\textbf{R})$). Residual are uncorrelated,
hence \textbf{R} is equivalent to $\sigma^{2}$\textbf{I}, where
\textbf{I} is the identity matrix. The variance matrix \textbf{V}
can therefore be written as;

\begin{equation}
\textbf{V}  = ZGZ^{T} + \textbf{R}
\end{equation}

%\subsection{Estimators and Predictors}

The best linear unbiased predictor (BLUP) is used to estimating
random effects, i.e to derive \textbf{u}. The best linear unbiased
estimator (BLUE) is used to estimate the fixed effects,
\textbf{b}. They were formulated in a paper by \cite{Henderson59},
which provides the derivations of both. Inferences about fixed
effects have come to be called `estimates', whereas inferences
about random effects have come be called `predictions`. hence the
naming of BLUP is to reinforce distinction between the two , but
it is essentially the same principal involved in both cases
\citep{Robinson}. The BLUE of \textbf{b}, and the BLUP of
\textbf{u} can be shown to be;

\begin{equation}
\hat{b} = (X^{T}V^{-1}X)^{-1}X^{T}V^{-1}y
\end{equation}
\begin{equation}
\hat{u} = GZ^{T}V^{-1}(y-X\hat{b})
\end{equation}

The practical application of both expressions requires that the variance components be known. An estimate for the variance
components must be derived to  either maximum likelihood (ML) or more commonly restricted maximum likelihood (REML).

Importantly calculations based on the above formulae require the calculation of the inverse of \textbf{V}. In simple examples
$V^{-1}$ is a straightforward calculation, but with higher dimensions it becomes a very complex calculation.




\subsection
The classical model is based on measurements $y_{mi}$ by method $m=1,2$ on item $i = 1,2 \ldots$

\[y_{mi} + \alpha_{m} + \mu_{i} + e_{mi}\]

\[e_{mi} \sim \mathcal{n} (0,\sigma^2_m)\]

Even though the separate variances can not be identified, their sum can be estimated by the empirical variance of the differences.

Like wise the separate $\alpha$ can not be estimated, only theiir difference can be estimated as $\bar{D}$


