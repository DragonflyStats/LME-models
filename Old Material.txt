
\chapter{Model Diagnostics}
%---------------------------------------------------------------------------%
%1.1 Introduction to Influence Analysis
%1.2 Extension of techniques to LME Models
%1.3 Residual Diagnostics
%1.4 Standardized and studentized residuals
%1.5 Covariance Parameters
%1.6 Case Deletion Diagnostics
%1.7 Influence Analysis
%1.8 Terminology for Case Deletion
%1.9 Cook's Distance (Classical Case)
%1.10 Cook's Distance (LME Case)
%1.11 Likelihood Distance
%1.12 Other Measures
%1.13 CPJ Paper
%1.14 Matrix Notation of Case Deletion
%1.15 CPJ's Three Propositions
%1.16 Other measures of Influence
%---------------------------------------------------------------------------%
\section{Introduction}%1.1
In classical linear models model diagnostics have been become a required part of any statistical analysis, and the methods are commonly available in statistical packages and standard textbooks on applied regression. However it has been noted by several papers that model diagnostics do not often accompany LME model analyses.
Model diagnostic techniques determine whether or not the distributional assumptions are satisfied, and to assess the influence of unusual observations.

\subsection{Model Data Agreement} %1.1.1
\citet{schabenberger} describes the examination of model-data agreement as comprising several elements; residual analysis, goodness of fit, collinearity diagnostics and influence analysis.

\subsection{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm

The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 





%---------------------------------------------------------------------------%
\newpage
\section{Extension of techniques to LME Models} %1.2

Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models.Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.

Beckman, Nachtsheim and Cook (1987) \citet{Beckman} applied the \index{local influence}local influence method of Cook (1986) to the analysis of the linear mixed model.

While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.

If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following

\begin{itemize}
\item the estimates of fixed effects,
\item the estimates of the precision of the fixed effects,
\item the estimates of the covariance parameters,
\item the estimates of the precision of the covariance parameters,
\item fitted and predicted values.
\end{itemize}


%---------------------------------------------------------------------------%
\newpage
\section{Covariance Parameters} %1.5
The unknown variance elements are referred to as the covariance parameters and collected in the vector $\theta$.
% - where is this coming from?
% - where is it used again?
% - Has this got anything to do with CovTrace etc?
%---------------------------------------------------------------------------%



\subsection{Methods and Measures}
The key to making deletion diagnostics useable is the development of efficient computational formulas, allowing one to obtain the \index{case deletion diagnostics} case deletion diagnostics by making use of basic building blocks, computed only once for the full model.

\citet{Zewotir} lists several established methods of analyzing influence in LME models. These methods include \begin{itemize}
\item Cook's distance for LME models,
\item \index{likelihood distance} likelihood distance,
\item the variance (information) ration,
\item the \index{Cook-Weisberg statistic} Cook-Weisberg statistic,
\item the \index{Andrews-Prebigon statistic} Andrews-Prebigon statistic.
\end{itemize}







\chapter{Zewotir's Paper}

% 2.1 Efficient Updating Theorem
% 2.2 Zewotir Measures of Influence in LME Models (section 4 of paper)
% 2.3 Computation and Notation 
% 2.4 Measures 2
%2.5 Haslett Analysis

\section{Efficient Updating Theorem} %2.1
\citet{Zewotir} describes the basic theorem of efficient updating.
\begin{itemize}
\item \[ m_i = {1 \over c_{ii}}\]
%\item
%item
%\item
\end{itemize}

%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------Chapter 3------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%

\chapter{Application to Method Comparison Studies} % Chapter 4


%---------------------------------------------------------------------------%
% - 1. Application to MCS
% - 2. Grubbs' Data
% - 3. R implementation
% - 4. Influence measures using R
%---------------------------------------------------------------------------%

\section{Application to MCS} %4.1

Let $\hat{\beta}$ denote the least square estimate of $\beta$
based upon the full set of observations, and let
$\hat{\beta}^{(k)}$ denoted the estimate with the $k^{th}$ case
excluded.


\section{Grubbs' Data} %4.2

For the Grubbs data the $\hat{\beta}$ estimated are
$\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ respectively. Leaving the
fourth case out, i.e. $k=4$ the corresponding estimates are
$\hat{\beta}_{0}^{-4}$ and $\hat{\beta}_{1}^{-4}$


\begin{equation}
Y^{-Q} = \hat{\beta}^{-Q}X^{-Q}
\end{equation}

When considering the regression of case-wise differences and averages, we write $D^{-Q} = \hat{\beta}^{-Q}A^{-Q}$


\newpage

\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & F & C & D & A \\
  \hline
1 & 793.80 & 794.60 & -0.80 & 794.20 \\
  2 & 793.10 & 793.90 & -0.80 & 793.50 \\
  3 & 792.40 & 793.20 & -0.80 & 792.80 \\
  4 & 794.00 & 794.00 & 0.00 & 794.00 \\
  5 & 791.40 & 792.20 & -0.80 & 791.80 \\
  6 & 792.40 & 793.10 & -0.70 & 792.75 \\
  7 & 791.70 & 792.40 & -0.70 & 792.05 \\
  8 & 792.30 & 792.80 & -0.50 & 792.55 \\
  9 & 789.60 & 790.20 & -0.60 & 789.90 \\
  10 & 794.40 & 795.00 & -0.60 & 794.70 \\
  11 & 790.90 & 791.60 & -0.70 & 791.25 \\
  12 & 793.50 & 793.80 & -0.30 & 793.65 \\
   \hline
\end{tabular}
\end{center}
\end{table}


\newpage

\begin{equation}
Y^{(k)} = \hat{\beta}^{(k)}X^{(k)}
\end{equation}

Consider two sets of measurements , in this case F and C , with the vectors of case-wise averages $A$ and case-wise differences $D$ respectively. A regression model of differences on averages can be fitted with the view to exploring some characteristics of the data.

When considering the regression of case-wise differences and averages, we write

\begin{equation}
D^{-Q} = \hat{\beta}^{-Q}A^{-Q}
\end{equation}
Let $\hat{\beta}$ denote the least square estimate of $\beta$ based upon the full set of observations, and let $\hat{\beta}^{(k)}$ denoted the estimate with the $k^{th}$ case excluded.

For the Grubbs data the $\hat{\beta}$ estimated are $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ respectively. Leaving the
fourth case out, i.e. $k=4$ the corresponding estimates are $\hat{\beta}_{0}^{-4}$ and $\hat{\beta}_{1}^{-4}$

\begin{equation}
Y^{(k)} = \hat{\beta}^{(k)}X^{(k)}
\end{equation}

Consider two sets of measurements , in this case F and C , with the vectors of case-wise averages $A$ and case-wise differences $D$ respectively. A regression model of differences on averages can be fitted with the view to exploring some characteristics of the data.

\begin{verbatim}
Call: lm(formula = D ~ A)

Coefficients: (Intercept)            A
  -37.51896      0.04656

\end{verbatim}




When considering the regression of case-wise differences and averages, we write

\begin{equation}
D^{-Q} = \hat{\beta}^{-Q}A^{-Q}
\end{equation}



\subsection{Influence measures using R} %4.4
\texttt{R} provides the following influence measures of each observation.

%Influence measures: This suite of functions can be used to compute
%some of the regression (leave-one-out deletion) diagnostics for
%linear and generalized linear models discussed in Belsley, Kuh and
% Welsch (1980), Cook and Weisberg (1982)



\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
 & dfb.1\_ & dfb.A & dffit & cov.r & cook.d & hat \\
  \hline
1 & 0.42 & -0.42 & -0.56 & 1.13 & 0.15 & 0.18 \\
  2 & 0.17 & -0.17 & -0.34 & 1.14 & 0.06 & 0.11 \\
  3 & 0.01 & -0.01 & -0.24 & 1.17 & 0.03 & 0.08 \\
  4 & -1.08 & 1.08 & 1.57 & 0.24 & 0.56 & 0.16 \\
  5 & -0.14 & 0.14 & -0.24 & 1.30 & 0.03 & 0.13 \\
  6 & -0.00 & 0.00 & -0.11 & 1.31 & 0.01 & 0.08 \\
  7 & -0.04 & 0.04 & -0.08 & 1.37 & 0.00 & 0.11 \\
  8 & 0.02 & -0.02 & 0.15 & 1.28 & 0.01 & 0.09 \\
  9 & 0.69 & -0.68 & 0.75 & 2.08 & 0.29 & 0.48 \\
  10 & 0.18 & -0.18 & -0.22 & 1.63 & 0.03 & 0.27 \\
  11 & -0.03 & 0.03 & -0.04 & 1.53 & 0.00 & 0.19 \\
  12 & -0.25 & 0.25 & 0.44 & 1.05 & 0.09 & 0.12 \\
   \hline
\end{tabular}
\end{center}
\end{table}



%-------------------------------------------------------------------------------------------------------%
\chapter{Appendices} % Chapter 5
%---------------------------------------------------------------------------------------------------------%
% Appendices
% - The Hat Matrix (5.1)
% - Sherman Morrison Woodbury Formula (5.2)
% -  Hat Matrix applied to MCS (5.3)
% - Cross Validation (Updating standard deviation) (5.4)
% - Updating Estimates (5.5)
% - Lesaffre's paper (5.6)
%---------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------%
\newpage
\section{The Hat Matrix} %5.1

The projection matrix $H$ (also known as the hat matrix), is a
well known identity that maps the fitted values $\hat{Y}$ to the
observed values $Y$, i.e. $\hat{Y} = HY$.

\begin{equation}
H =\quad X(X^{T}X)^{-1}X^{T}
\end{equation}

$H$ describes the influence each observed value has on each fitted
value. The diagonal elements of the $H$ are the `leverages', which
describe the influence each observed value has on the fitted value
for that same observation. The residuals ($R$) are related to the
observed values by the following formula:
\begin{equation}
R = (I-H)Y
\end{equation}

The variances of $Y$ and $R$ can be expressed as:
\begin{eqnarray}
\mbox{var}(Y) = H\sigma^{2} \nonumber\\
\mbox{var}(R) = (I-H)\sigma^{2}
\end{eqnarray}

Updating techniques allow an economic approach to recalculating
the projection matrix, $H$, by removing the necessity to refit the
model each time it is updated. However this approach is known for
numerical instability in the case of down-dating.

\section{Sherman Morrison Woodbury Formula} % 5.2

The `Sherman Morrison Woodbury' Formula is a well known result in
linear algebra;
\begin{equation}
(A+a^{T}B)^{-1} \quad = \quad A^{-1}-
A^{-1}a^{T}(I-bA^{-1}a^{T})^{-1}bA^{-1}
\end{equation}

This result is highly useful for analyzing regression diagnostics,
and for matrices inverses in general. Consider a $p \times p$
matrix $X$, from which a row $x_{i}^{T}$ is to be added or
deleted. \citet{CookWeisberg} sets $A = X^{T}X$, $a=-x_{i}^{T}$
and $b=x_{i}^{T}$, and writes the above equation as

\begin{equation}
(X^{T}X \pm x_{i}x_{i}^{T})^{-1} = \quad(X^{T}X )^{-1} \mp \quad
\frac{(X^{T}X)^{-1}(x_{i}x_{i}^{T}(X^{T}X)^{-1}}{1-x_{i}^{T}(X^{T}X)^{-1}x_{i}}
\end{equation}

The projection matrix $H$ (also known as the hat matrix), is a
well known identity that maps the fitted values $\hat{Y}$ to the
observed values $Y$, i.e. $\hat{Y} = HY$.

\begin{equation}
H =\quad X(X^{T}X)^{-1}X^{T}
\end{equation}

$H$ describes the influence each observed value has on each fitted value. The diagonal elements of the $H$ are the `leverages', which describe the influence each observed value has on the fitted value for that same observation. The residuals ($R$) are related to the observed values by the following formula:
\begin{equation}
R = (I-H)Y
\end{equation}

The variances of $Y$ and $R$ can be expressed as:
\begin{eqnarray}
\mbox{var}(Y) = H\sigma^{2} \nonumber\\
\mbox{var}(R) = (I-H)\sigma^{2}
\end{eqnarray}

Updating techniques allow an economic approach to recalculating the projection matrix, $H$, by removing the necessity to refit the model each time it is updated. However this approach is known for
numerical instability in the case of down-dating.



\subsection{Hat Values for MCS regression}

With A as the averages and D as the casewise differences.
\begin{verbatim}
fit = lm(D~A)
\end{verbatim}

\begin{displaymath}
H = A \left(A^\top  A\right)^{-1} A^\top ,
\end{displaymath}




\printindex
\bibliographystyle{chicago}
\bibliography{DB-txfrbib}
\newpage
\bibliography{DB-txfrbib}
\end{document}
